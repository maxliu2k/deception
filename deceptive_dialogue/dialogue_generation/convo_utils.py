import os
import glob
import re
from openai import OpenAI
import openai

import json
import time
from absl import flags
from api_cost import api_cost
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

_VLLM_AVAILABLE = False
try:
    from vllm import LLM, SamplingParams
    import ray
    _VLLM_AVAILABLE = True
except ImportError as err:
    print(err)
    _VLLM_AVAILABLE = False


FLAGS = flags.FLAGS
_default_key = None
try:
    _default_key = glob.glob(os.path.abspath('../../marwa/*openai*'))[0]
except IndexError:
    _default_key = None
flags.DEFINE_string('openai_key', _default_key, 'path to openai key')
# flags.DEFINE_string('tmp_dir', glob.glob(os.path.abspath('../../*tmp*'))[0], 'path to tmp directory to download vLLM models')

flags.DEFINE_string('model', 'gpt-3.5-turbo', 'gpt-3.5-turbo / default, gpt-4-turbo, gpt-4o, gpt-3.5-turbo-instruct, meta-llama/Llama-2-70b-hf')
flags.DEFINE_integer('iterations', 1, 'integer number of iterations (default 1)')
flags.DEFINE_boolean('verbose', True, 'actually print out json? (will still show tqdm bar if False)')
flags.DEFINE_boolean('write', False, 'write to file? name format is generated as output_gym_<exo/exn/s>_<once/turns>_<sof1>_<sof2>_<iterations>_<taxonomy>.json')
flags.DEFINE_string('persuasion_taxonomy', 'none', 'what kind of persuasion taxonomy to use, if any? (none/default, full, reduced, no_examples)')
flags.DEFINE_integer('convo_length_limit', 0, 'upper limit of conversation length (0/default means no limit)')
flags.DEFINE_integer('gpus', 1, 'number of gpus to run inference on (1/default)')
flags.DEFINE_integer('seed', 0, 'seed used for random point generation')
flags.DEFINE_string('model_dir', '/raid/users/ryany2/models', 'directory to download model weights, use gcsfuse to mount bucket on TPUs')
flags.DEFINE_integer('max_tokens', None, 'upper limit on tokens generated by model (256/default)')
flags.DEFINE_boolean('deception', False, 'whether to use deceptive prompting (False/default)')
flags.DEFINE_string('gen_format', 'turns', 'in turns (turns / default), all at once (once)')
flags.DEFINE_string('theory_of_mind', 'none', 'which theory of mind experiment to use (none/default, theory_of_mind, half_theory_of_mind)')
flags.DEFINE_boolean('tpu', False, 'whether we are using the TPUs for vLLM inference (False/default)')
flags.DEFINE_integer('max_guess_tries', 10, 'how many times to rerun guess generation if output is malformed (10/default)')
flags.DEFINE_boolean('half_agent', False, 'one agent prompted with deception/persuasion taxonomy and the other is not (False/default)')
flags.DEFINE_string('config_file', None, 'config file to set up experiment, overrides flags (None/default)')
flags.DEFINE_boolean('chain_of_thought', False, 'whether to query each agent each turn on why they responded in the way they did (False/default)')
flags.DEFINE_boolean('fp8', False, 'low memory Llama-3.1-70B generation')
flags.DEFINE_boolean('vllm', True, 'whether to load vllm (True/default)')
flags.DEFINE_boolean('thinking', True, 'whether to use thinking tokens in the response (True/default)')
flags.DEFINE_string('listener_model', None, 'whether to use a listener model (None/default)')

flags.DEFINE_boolean('all_costs', False, 'convince the persuader(s) to persuade at all costs (False/default)')
flags.DEFINE_boolean('honest', False, 'explicitly prompt the persuader to be honest (False/default)')
flags.DEFINE_boolean('truthful', False, 'explicitly prompt the persuader to be cooperative and truthful (False/default)')
flags.DEFINE_boolean('simple', False, 'use short responses and hard turn limit (False/default)')

flags.DEFINE_string('filename', None, 'run metrics on a particular filename (None/default specifies all filenames in all folders)')
config = {}
# see reset_stats for values
stats = {}

def set_flag_variables():
    for flag in FLAGS:
        config[flag] = FLAGS[flag].value
    if config.get('vllm') and not _VLLM_AVAILABLE:
        # Fall back cleanly when vLLM isn't installed.
        config['vllm'] = False

def extract_ordered_tuples(text):
    pattern = r'\(([^)]+)\).*\s*<EOD>'
    match = re.findall(pattern, text)
    if match:
        ordered_tuples = [tuple(map(int, lst.split(','))) for lst in match]
        return ordered_tuples
    else:
        return None

def extract_model_name(text):
    pattern = r'\/([^\/]+)$'
    match = re.findall(pattern, text)
    if match:
        return match[0]
    else:
        return text

def load_stats_file(write_file):
    '''
    Creates write_file path and file if it doesn't exist
    Otherwise load and determine index_offset of file
    Returns index_offset
    '''
    index_offset = -1
    if config['write']:
        os.makedirs(os.path.dirname(os.path.abspath(write_file)), exist_ok=True)
    if config['write'] and not os.path.exists(write_file):
        with open(write_file, "w") as f:
            print('written!!')
            json.dump([], f)
            index_offset = 0
    if config['write'] and index_offset == -1:
        with open(write_file, "r") as f:
                index_offset = len(json.load(f))
    return index_offset

def write_stats(write_file):    
    '''
    Appends stats to end of write file
    '''
    if config['verbose']:
        print("Writing to file!")
    with open(write_file, "r") as f:
        conversations = json.load(f)
    conversations.append(stats)
    with open(write_file, "w") as f:
        json.dump(conversations, f, indent=4)


vllm_alias = {
    'mistral': 'mistralai/Mistral-7B-v0.3',
    'mixtral': 'mistralai/Mixtral-8x7B-v0.1',
    'mistral-instruct': 'mistralai/Mistral-7B-Instruct-v0.3',
    'mixtral-instruct': 'mistralai/Mixtral-8x7B-Instruct-v0.1',

    'gemma': 'google/gemma-7b',
    'gemma-2-2b': 'google/gemma-2-2b',
    'gemma-2-2b-it': 'google/gemma-2b-it',
    'gemma-2-27b': 'google/gemma-2-27b',
    'gemma-2-27b-it': 'google/gemma-2-27b-it', # instruction tuned

    'Llama-3-70B': 'meta-llama/Meta-Llama-3-70B',
    'Llama-3-8B': 'meta-llama/Meta-Llama-3-8B',
    'Llama-3.1-405B': 'meta-llama/Meta-Llama-3.1-405B',
    'Llama-3.1-8B': 'meta-llama/Meta-Llama-3.1-8B',
    'Llama-3.1-70B': 'meta-llama/Meta-Llama-3.1-70B', 
    'Llama-3.1-405B-Instruct': 'meta-llama/Meta-Llama-3.1-405B-Instruct',
    'Llama-3.1-70B-Instruct': 'meta-llama/Meta-Llama-3.1-70B-Instruct',
    'Llama-3.3-70B-Instruct': 'meta-llama/Meta-Llama-3.3-70B-Instruct',
    'Llama-3.1-8B-Instruct': 'meta-llama/Meta-Llama-3.1-8B-Instruct',

    'Qwen3-0.6B': 'Qwen/Qwen3-0.6B',
    'Qwen3-1.7B': 'Qwen/Qwen3-1.7B',
    'Qwen3-4B': 'Qwen/Qwen3-4B',
    'Qwen3-8B': 'Qwen/Qwen3-8B',
    'Qwen3-14B': 'Qwen/Qwen3-14B',
    'Qwen3-30B': 'Qwen/Qwen3-30B-A3B',
    'Qwen3-32B': 'Qwen/Qwen3-32B',
    'Qwen3-235B': 'Qwen/Qwen3-235B-A22B',
    'phi-3.5-mini-instruct': 'microsoft/phi-3.5-mini-instruct'
}

llm = None

_OPENAI_MODELS = {
    "gpt-3.5-turbo",
    "gpt-4-turbo",
    "gpt-4o",
    "gpt-4o-mini",
    "gpt-5.2",
    "gpt-5-mini",
}

def _is_openai_model(model_name):
    return model_name in _OPENAI_MODELS

# run 'ray start --head' in bash first!
def setup_vllm():
    if _is_openai_model(config.get('model')):
        return
    if config['gpus'] > 1:
        ray.init(ignore_reinit_error=True) 

    global llm
    global tokenizer

    if config['model'] in vllm_alias:
        tokenizer = AutoTokenizer.from_pretrained(vllm_alias[config['model']])
    else:
        print('Attempting to use tokenizer for', config['model'])
        try:
            tokenizer = AutoTokenizer.from_pretrained(config['model'])
        except:
            print('Info: passing tokenizer setup')

    if config['vllm']:
        if config['model'] in vllm_alias:
            if config['fp8']:
                llm = LLM(model=vllm_alias[config['model']], tensor_parallel_size=config['gpus'], download_dir=config['model_dir'], gpu_memory_utilization=0.75)
            elif vllm_alias[config['model']] == 'meta-llama/Meta-Llama-3.1-70B' or vllm_alias[config['model']] == 'meta-llama/Meta-Llama-3.1-70B-Instruct':
                llm = LLM(model=vllm_alias[config['model']], tensor_parallel_size=config['gpus'], download_dir=config['model_dir'], gpu_memory_utilization=0.9, max_model_len=12880)
            else:
                llm = LLM(model=vllm_alias[config['model']], tensor_parallel_size=config['gpus'], download_dir=config['model_dir'])
        else:
            try:
                llm = LLM(model=config['model'], tensor_parallel_size=config['gpus'], download_dir=config['model_dir'])
            except:
                print('Info: Passing vllm setup')
    else:
        llm = AutoModelForCausalLM.from_pretrained(
            config['model'],
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            device_map="auto"
        )

def set_global_variables(config_file):
    with open(config_file, "r") as f:
            config_loaded = json.load(f)
    for key in config_loaded.keys():
        config[key] = config_loaded[key]

def init():
    global client
    set_flag_variables()
    if FLAGS.config_file:
        set_global_variables(FLAGS.config_file)
    #if config['tpu']:
        #prevent_cross_region(config['model_dir'], '/nfs/nfs2/users')
    if FLAGS.openai_key:
        with open(FLAGS.openai_key, 'r') as f:
            client = OpenAI(api_key=f.read().rstrip('\n'))
    else:
        client = None

def completion_create_helper(prompt, model):
    # limit prompt in all cases
    #if config['model'] not in vllm_models:
        # for some reason vLLM models simply repeat this last statement if present
    #    prompt += " Limit your answer to a maximum of 50 words!"
    global client
    global tokenizer
    ret = '' # return the output ret at the end and use to calculate cost

    if model == "gpt-3.5-turbo-instruct": # previously davinci, but that's now deprecated
        req = dict(
            model="gpt-3.5-turbo-instruct",
            prompt=prompt,
            temperature=0.8,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0,
        )
        if config['max_tokens'] is not None:
            req["max_tokens"] = config['max_tokens']
        ret = client.completions.create(**req)
        ret = ret.choices[0].text.strip()
        #find_line = ret.find("\n")
        #if find_line != -1:
            #ret = ret[:find_line]
    elif model in ["gpt-3.5-turbo", "gpt-4-turbo", "gpt-4o", 'gpt-4o-mini', "gpt-5.2", "gpt-5-mini"]:
        if config['gen_format'] == 'turns':
            req = dict(
                model=model,
                messages=[{"role": "user", "content": prompt}],
            )
            if config['max_tokens'] is not None:
                req["max_tokens"] = config['max_tokens']
            ret = client.chat.completions.create(**req)
            ret = ret.choices[-1].message.content
            # find_line = ret.find("\n")
            # if find_line != -1:
            #     ret = ret[:find_line]
        elif config['gen_format'] == 'once': # deprecated case, don't use
            req = dict(
                model=model,
                messages=[{"role": "user", "content": prompt}],
            )
            if config['max_tokens'] is not None:
                req["max_tokens"] = config['max_tokens']
            ret = client.chat.completions.create(**req)
            ret = ret.choices[-1].message.content
        else:
            raise IllegalFlagValueError
    elif config['vllm'] and _VLLM_AVAILABLE: # assume huggingface model in general case with full path
        sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=config['max_tokens'])
        messages = [
            {"role": "user", "content": prompt}
        ]
        if "Qwen3" in model:
            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=config['thinking'])
        elif tokenizer.chat_template:
            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        # if tokenizer.chat_template:
            # prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        output = llm.generate([prompt], sampling_params)
        if 'thinking' in config and config['thinking']:
            ret = output[0].outputs[0].text.split("</think>\n\n")[-1]
        else:
            ret = output[0].outputs[0].text
        
    else: # specify model path in model directory
        if 'Qwen3' in model:
            print("loaded Qwen3 tokenizer")
            tokenizer = AutoTokenizer.from_pretrained(model)
            # prompt = "<im_start>user\n" + prompt + "\n<im_end>\n<im_start>assistant\n<think>\n"
        messages = [{"role": "user", "content": prompt}]
        if 'Qwen3' in model:
            inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=config['thinking'])
            print("||||||||||||||||||||||||||||||")
            print(inputs)
            print("||||||||||||||||||||||||||||||")
        else:
            inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer(inputs, return_tensors='pt').to('cuda')
        # print("~~~~~~~~~~~~~~~~~~~~~~~~~")
        # # print inputs as plaintext
        # try:
        #     # extract first sequence, move to CPU and convert to list of ids
        #     input_ids = inputs['input_ids'][0]
        #     if hasattr(input_ids, 'cpu'):
        #         input_ids = input_ids.cpu().tolist()
        #     else:
        #         input_ids = list(input_ids)
        #     plaintext = tokenizer.decode(input_ids, skip_special_tokens=False)
        # except Exception:
        #     try:
        #         plaintext = tokenizer.batch_decode(inputs.get('input_ids'), skip_special_tokens=False)[0]
        #     except Exception:
        #         plaintext = str({k: (v.shape if hasattr(v, 'shape') else type(v)) for k, v in inputs.items()})
        # print(plaintext)
        # print("~~~~~~~~~~~~~~~~~~~~~~~~~")
        with torch.no_grad():
            output_ids = llm.generate(**inputs, max_new_tokens=8192)
        output_ids = output_ids[:, inputs.input_ids.shape[1]:]
        ret = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        print("~~~~~~~~~~~~~~~~~~~~~~~~~")
        print("Generated output: ", ret)
        print("~~~~~~~~~~~~~~~~~~~~~~~~~")
        if '</think>' in ret:
            ret = ret.split("</think>")[-1]
        if '<im_end>' in ret:
            ret = ret.split("<im_end>")[0]

    # if model not in vllm_alias:
    #     stats['running_cost_for_iteration'] += api_cost(prompt=prompt, answer=ret, model=model)
        # if config['verbose']:
        #     print(f"RUNNING COST FOR ITERATION SO FAR: {stats['running_cost_for_iteration']}")
    if config['verbose']:
        print("~~~~~~~~~~~~~~~~~~~~~~~~~")
        print("Generated output: ", ret)
        print("~~~~~~~~~~~~~~~~~~~~~~~~~")
    return ret

def completion_create(prompt, keep_trying=False, model=None):
    if model == None:
        model = config['model'] # default set to config['model']
    try:
        return completion_create_helper(prompt, model)
    except (openai.APIError, openai.OpenAIError) as e:
        print("ERROR", e)
        print("sleeping for 10 seconds.")
        time.sleep(10)
        if keep_trying:
            return completion_create(prompt, keep_trying)
        else:
            return None
