#!/usr/bin/env python3
"""
Verification script for generated JSONL training data
"""

import json

def verify_jsonl_files():
    print("=" * 80)
    print("VERIFICATION: DND JSONL Training Data")
    print("=" * 80)
    print()
    
    # Check train.jsonl
    print("1. TRAIN.JSONL")
    print("-" * 80)
    with open('train.jsonl', 'r') as f:
        train_lines = f.readlines()
    
    print(f"   Total examples: {len(train_lines)}")
    
    # Parse and analyze
    train_examples = [json.loads(line) for line in train_lines[:100]]
    
    # Check keys
    if train_examples:
        keys = set(train_examples[0].keys())
        print(f"   Keys per example: {sorted(keys)}")
    
    # Score distribution
    scores = [json.loads(line)['score'] for line in train_lines]
    score_0 = sum(1 for s in scores if s == 0)
    score_1 = sum(1 for s in scores if s == 1)
    print(f"   Score distribution:")
    print(f"     - Score 0: {score_0:,} ({score_0/len(scores)*100:.1f}%)")
    print(f"     - Score 1: {score_1:,} ({score_1/len(scores)*100:.1f}%)")
    
    # Agent distribution
    agent1_count = sum(1 for line in train_lines if 'Agent 1: ' in json.loads(line)['in_text'][-50:])
    agent2_count = sum(1 for line in train_lines if 'Agent 2: ' in json.loads(line)['in_text'][-50:])
    print(f"   Agent distribution:")
    print(f"     - Agent 1: {agent1_count:,} ({agent1_count/len(train_lines)*100:.1f}%)")
    print(f"     - Agent 2: {agent2_count:,} ({agent2_count/len(train_lines)*100:.1f}%)")
    
    print()
    
    # Check test.jsonl
    print("2. TEST.JSONL")
    print("-" * 80)
    with open('test.jsonl', 'r') as f:
        test_lines = f.readlines()
    
    print(f"   Total examples: {len(test_lines)}")
    
    # Score distribution
    scores_test = [json.loads(line)['score'] for line in test_lines]
    score_0_test = sum(1 for s in scores_test if s == 0)
    score_1_test = sum(1 for s in scores_test if s == 1)
    print(f"   Score distribution:")
    print(f"     - Score 0: {score_0_test:,} ({score_0_test/len(scores_test)*100:.1f}%)")
    print(f"     - Score 1: {score_1_test:,} ({score_1_test/len(scores_test)*100:.1f}%)")
    
    print()
    
    # Check metadata.json
    print("3. METADATA.JSON")
    print("-" * 80)
    with open('metadata.json', 'r') as f:
        metadata = json.load(f)
    
    print(f"   Total entries: {len(metadata):,}")
    
    if metadata:
        first_key = list(metadata.keys())[0]
        first_meta = metadata[first_key]
        print(f"   Metadata fields: {sorted(first_meta.keys())}")
    
    print()
    
    # Sample example
    print("4. SAMPLE TRAINING EXAMPLE")
    print("-" * 80)
    if train_lines:
        example = json.loads(train_lines[0])
        print(f"   Score: {example['score']}")
        print()
        print("   Input (last 300 chars):")
        print(f"   ...{example['in_text'][-300:]}")
        print()
        print("   Output (first 200 chars):")
        print(f"   {example['out_text'][:200]}...")
    
    print()
    
    # Verification checks
    print("5. VERIFICATION CHECKS")
    print("-" * 80)
    checks = [
        ("Train/test split ratio", abs(len(train_lines) / len(test_lines) - 4.0) < 0.1),
        ("All examples have required keys", all(set(json.loads(line).keys()) == {'in_text', 'out_text', 'score'} for line in train_lines[:100])),
        ("Scores are binary (0 or 1)", all(json.loads(line)['score'] in [0, 1] for line in train_lines[:100])),
        ("Both agents represented", agent1_count > 0 and agent2_count > 0),
        ("Both scores represented", score_0 > 0 and score_1 > 0),
        ("Metadata entries exist", len(metadata) > 0),
    ]
    
    for check_name, passed in checks:
        status = "✓" if passed else "✗"
        print(f"   {status} {check_name}")
    
    print()
    print("=" * 80)
    print("VERIFICATION COMPLETE!")
    print("=" * 80)

if __name__ == '__main__':
    verify_jsonl_files()
